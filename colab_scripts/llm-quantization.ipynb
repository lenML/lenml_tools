{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个脚本用于在 colab 上使用 llama.cpp 对模型进行量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖\n",
    "\n",
    "# build llama-cpp\n",
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "!mkdir llama.cpp/build && cd llama.cpp/build && cmake .. && cmake --build . --config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置，使用 colab 参数\n",
    "source_repo_name = \"lenML/aya-expanse-8b-abliterated\"\n",
    "model_name = \"aya-expanse-8b-abliterated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "base_model = \"./original_model/\"\n",
    "snapshot_download(repo_id=source_repo_name, local_dir=base_model, ignore_patterns=[\"*.pth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_option = \"Q4_K_M\"\n",
    "output_model_file = f\"/content/models/{model_name}-{quant_option}.gguf\"\n",
    "\n",
    "!cd llama.cpp/build/bin && ./llama-quantize /content/models/model_FP16.gguf {output_model_file} {quant_option}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install llama-cpp-python\n",
    "!pip install llama-cpp-python==0.2.85\n",
    "\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"max_tokens\":200,\n",
    "    \"echo\":False,\n",
    "    \"top_k\":1\n",
    "}\n",
    "\n",
    "prompt = \"Q: \"\n",
    "res = llm(prompt, **generation_kwargs)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "target_repo_name = \"lenML/aya-expanse-8b-abliterated-GGUF\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
